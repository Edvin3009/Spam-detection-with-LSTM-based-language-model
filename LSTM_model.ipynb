{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1802686",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f84a39b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1215305e",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5f95bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamOrHamDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, seq_length):\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Read CSV file and split into emails and labels\n",
    "        data = pd.read_csv(csv_file)\n",
    "        self.emails = data[\"email\"].astype(str).tolist()\n",
    "        self.labels = data[\"label\"].astype(int).tolist()\n",
    "\n",
    "        self.class_counts = Counter(self.labels)\n",
    "        print(f\"\\nClass distribution: {self.class_counts}\")\n",
    "        print(f\"Spam percentage: {self.class_counts[1]/len(self.labels)*100:.1f}%\")\n",
    "        print(f\"Ham percentage: {self.class_counts[0]/len(self.labels)*100:.1f}%\")\n",
    "\n",
    "        # Tokenize emails\n",
    "        self.tokenized_emails = [email.split() for email in self.emails]\n",
    "        \n",
    "        # Create vocabulary from all words in emails\n",
    "        self.vocab = self.obtainUniqueWords(self.tokenized_emails)\n",
    "        self.id2word = {i: w for i, w in enumerate(self.vocab)}\n",
    "        self.word2id = {w: i for i, w in enumerate(self.vocab)}\n",
    "        self.listOfIds = [self.word2id[w] for w in self.vocab]\n",
    "\n",
    "        # Encode tokenized emails\n",
    "        self.encoded_emails = [self.encode(email) for email in self.tokenized_emails]\n",
    "\n",
    "\n",
    "    def obtainUniqueWords(self, tokenized_texts):\n",
    "        # Find all unique words\n",
    "        wordCounts = Counter()\n",
    "        for words in tokenized_texts:\n",
    "            wordCounts.update(words)\n",
    "\n",
    "        unique_words = sorted(wordCounts, key=wordCounts.get, reverse=True)\n",
    "        # Add padding token and unknown token\n",
    "        return [\"<PAD>\", \"<UNK>\"] + unique_words\n",
    "    \n",
    "    def encode(self, text):\n",
    "        ids = [self.word2id.get(token, 1) for token in text]\n",
    "\n",
    "        # Pad or truncate to seq_length size\n",
    "        if len(ids) > self.seq_length:\n",
    "            ids = ids[:self.seq_length]\n",
    "        else:\n",
    "            ids += [0] * (self.seq_length - len(ids))\n",
    "\n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            self.encoded_emails[index],\n",
    "            torch.tensor(self.labels[index], dtype=torch.long)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276f62c1",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d00e21e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model(nn.Module):\n",
    "\n",
    "    def __init__(self, num_embeddings, embedding_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(x)\n",
    "        last_hidden = hidden[-1, :, :]\n",
    "        out = self.fc(last_hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1539b9",
   "metadata": {},
   "source": [
    "### Train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eaba5dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, epochs):\n",
    "    model.train()\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for x, target in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(x)\n",
    "            loss = criterion(output.squeeze(), target.float())\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "def test_model(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, target in dataloader:\n",
    "            output = model(x)\n",
    "            pred = (torch.sigmoid(output) > 0.5).int()\n",
    "            \n",
    "            predictions.extend(pred)\n",
    "            labels.extend(target)\n",
    "\n",
    "    print(classification_report(labels, predictions, target_names=['Ham', 'Spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661b671",
   "metadata": {},
   "source": [
    "### Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fd24042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 1  # For binary classification\n",
    "NUM_LAYERS = 2\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "SEQ_LENGTH = 75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dde258",
   "metadata": {},
   "source": [
    "### Train and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0e08ba59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution: Counter({0: 4827, 1: 747})\n",
      "Spam percentage: 13.4%\n",
      "Ham percentage: 86.6%\n",
      "\n",
      "Dataset:\n",
      "Vocabulary size: 15735\n",
      "Number of samples: 5574\n",
      "\n",
      "Model Architecture:\n",
      "LSTM_Model(\n",
      "  (embedding): Embedding(15735, 100)\n",
      "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "Trainable parameters: 1,823,485\n",
      "\n",
      "Training for 25 epochs...\n",
      "Epoch [1/25] completed. Average Loss: 0.4008\n",
      "Epoch [2/25] completed. Average Loss: 0.3896\n",
      "Epoch [3/25] completed. Average Loss: 0.3899\n",
      "Epoch [4/25] completed. Average Loss: 0.3886\n",
      "Epoch [5/25] completed. Average Loss: 0.3879\n",
      "Epoch [6/25] completed. Average Loss: 0.3890\n",
      "Epoch [7/25] completed. Average Loss: 0.3870\n",
      "Epoch [8/25] completed. Average Loss: 0.3890\n",
      "Epoch [9/25] completed. Average Loss: 0.3876\n",
      "Epoch [10/25] completed. Average Loss: 0.3878\n",
      "Epoch [11/25] completed. Average Loss: 0.3881\n",
      "Epoch [12/25] completed. Average Loss: 0.3864\n",
      "Epoch [13/25] completed. Average Loss: 0.3873\n",
      "Epoch [14/25] completed. Average Loss: 0.3874\n",
      "Epoch [15/25] completed. Average Loss: 0.3874\n",
      "Epoch [16/25] completed. Average Loss: 0.3070\n",
      "Epoch [17/25] completed. Average Loss: 0.1451\n",
      "Epoch [18/25] completed. Average Loss: 0.1142\n",
      "Epoch [19/25] completed. Average Loss: 0.0477\n",
      "Epoch [20/25] completed. Average Loss: 0.0272\n",
      "Epoch [21/25] completed. Average Loss: 0.0151\n",
      "Epoch [22/25] completed. Average Loss: 0.0070\n",
      "Epoch [23/25] completed. Average Loss: 0.0018\n",
      "Epoch [24/25] completed. Average Loss: 0.0012\n",
      "Epoch [25/25] completed. Average Loss: 0.0007\n",
      "\n",
      "Testing results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.98      0.98      0.98      1435\n",
      "        Spam       0.87      0.90      0.89       238\n",
      "\n",
      "    accuracy                           0.97      1673\n",
      "   macro avg       0.93      0.94      0.93      1673\n",
      "weighted avg       0.97      0.97      0.97      1673\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = SpamOrHamDataset(\"spam_or_not_spam_SMS.csv\", seq_length=SEQ_LENGTH)\n",
    "\n",
    "# Get vocabulary size from dataset\n",
    "NUM_EMBEDDINGS = len(dataset.vocab)\n",
    "print(\"\\nDataset:\")\n",
    "print(f\"Vocabulary size: {NUM_EMBEDDINGS}\")\n",
    "print(f\"Number of samples: {len(dataset)}\")\n",
    "\n",
    "# Split into train and test data\n",
    "train_size = int(0.7 * len(dataset))\n",
    "test_size = (len(dataset) - train_size)\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize model using hyperparameters\n",
    "model = LSTM_Model(\n",
    "    num_embeddings=NUM_EMBEDDINGS,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    num_layers=NUM_LAYERS\n",
    ")\n",
    "\n",
    "# Print model architecture\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTrainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Train model\n",
    "print(f\"\\nTraining for {EPOCHS} epochs...\")\n",
    "train_model(model, train_loader, EPOCHS)\n",
    "\n",
    "# Test model\n",
    "print(\"\\nTesting results:\")\n",
    "test_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
